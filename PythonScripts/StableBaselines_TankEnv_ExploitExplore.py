import os
os.environ['MKL_THREADING_LAYER'] = 'GNU'
import argparse
from config_gen import config_gen
import json
from elo import elo_prob
from random import shuffle, randint, choice, uniform
from stable_baselines3 import PPO
import subprocess

ID = 0
ELO = 1
    
# Setup command line arguments
parser = argparse.ArgumentParser()
parser.add_argument("game_path", type=str, help="File path of game executable")
parser.add_argument("game_config_file_path", type=str, help="File path of game config file")
parser.add_argument("eval_script", type=str, help="Evaluation script path")
parser.add_argument("model_dir", type=str, help="Base directory for agent models")
parser.add_argument("pop_file_path", type=str, help="Path to file that contains IDs of agents in population to train")
parser.add_argument("noun_file_path", type=str, help="Path to noun file used to generate names")
parser.add_argument("adj_file_path", type=str, help="Path to adj file used to generate names")
parser.add_argument("--win_thresh", type=float, default=.7, help = "Threshold value for win rate that determines when superior models replace inferior ones")
parser.add_argument("--num_trials", type=int, default=100, help="Number of trials for each pair in population to play out when evaluating for replacement")
parser.add_argument("--min_step", type=int, default=1000000, help="Minimum number of steps before agents can be considered for replacement")
parser.add_argument("--gamelog", type=str, default="gamelog.txt", help="Log file to direct game logging to")
parser.add_argument("--num_envs", type=int, default=1, help="Number of environments to run concurrently")
args = parser.parse_args()
print(args)

def get_reward_clear_stats(agent_id):
    agent_stats_file_path = args.model_dir + remove_steps(agent_id) + "/stats.json"
    with open(agent_stats_file_path, 'r') as agent_stats_file:
        agent_stats = json.load(agent_stats_file)
    last_reward = agent_stats["performance"]["avg_reward"][-1]
    agent_stats["performance"]["avg_reward"] = agent_stats["performance"]["avg_reward"][:-1]
    agent_stats["performance"]["avg_steps"] = agent_stats["performance"]["avg_steps"][:-1]
    agent_stats["performance"]["trained_steps"] = agent_stats["performance"]["trained_steps"][:-1]
    with open(agent_stats_file_path, 'w') as agent_stats_file:
        json.dump(agent_stats, agent_stats_file, indent=4)
    return (last_reward + 1.) / 2.
    
def remove_steps(id):
    return "".join(id.split('_')[:-1])

def evaluate(agent_a, agent_b):
    # Establish opponents file so agent_a plays against agent_b
    with open(args.model_dir + remove_steps(agent_a[ID]) + "/opponents.txt", 'w') as opp_file:
        opp_file.write(agent_b[ID])
    # Setup game for evaluation
    #config_gen(args.game_config_file_path, random_start=False)
    # Loop forever so that if system fails, that error will keep repeating (for debugging purposes)
    # If that error only happens occasionaly, then this loop will be broken
    while True:
        # Run game
        with open(os.path.expanduser(args.gamelog), 'w') as gl:
            game_ps = []
            for i in range(args.num_envs):
                game_cmd_list = [args.game_path, str(50000+i)]
                #config_gen(args.game_config_file_path, random_start=args.rs, port=(50000+args.idx)+(i*args.part))
                game_p = subprocess.Popen(game_cmd_list, stdout=gl, stderr=gl)
                game_ps.append(game_p)
            # Execute evaluation script
            cmd_list = ["python", args.eval_script,
                        args.model_dir, remove_steps(agent_a[ID]),
                        "--num_trials", str(args.num_trials),
                        "--num_envs", str(args.num_envs)]
            with open(os.path.expanduser(args.model_dir + agent_a[ID].split('_')[0] + "/replace_log.txt"), 'a') as rl:
                print("Starting EE evalulation of " + agent_a[ID] + " vs " + agent_b[ID] + "\n")
                eval_p = subprocess.Popen(cmd_list, stdout=rl, stderr=rl)
                eval_return = eval_p.wait()
                print("Ending EE evaluation with exit code: " + str(eval_return) + "\n")
            for game_p in game_ps:
                game_p.wait()
        if eval_return in [0, -6, -7, -11]:
            break
        else:
            print("Had an exit code of", eval_return, "so redoing EE of", agent_a[ID] + " vs " + agent_b[ID])
    return get_reward_clear_stats(agent_a[ID])
    
def gen_name():
    with open(args.noun_file_path, 'r') as noun_file, open(args.adj_file_path, 'r') as adj_file:
        while True:
            name = choice(adj_file.readlines()).strip('\n').capitalize()+\
                choice(noun_file.readlines()).strip('\n').capitalize()+str(randint(0,100))
            if not os.path.isdir(args.model_dir + name):
                return name

def init_stats():
    model_stats = {
        "num_steps":0,
        "performance": {
            "avg_reward":[],
            "avg_steps":[],
            "trained_steps":[]
        },
        "elo": {
            "value": [1000],
            "steps": [0]
        },
        "parent": None,
        "matching_agent": None,
        "nemesis": False,
        "survivor": False
    }
    return model_stats
                
def child_agent(source_agent, gen_nemesis=False, gen_survivor=False):
    print("SOURCE AGENT", source_agent)
    name = gen_name()
    nemesis_name = name + "-nemesis" if gen_nemesis else None
    survivor_name = name + "-survivor" if gen_survivor else None
    print("NAME", name)
# Child agent
    src_model_file_path = args.model_dir + remove_steps(source_agent[ID]) + "/" + source_agent[ID]
    model = PPO.load(src_model_file_path)
    # Exploration - for now just altering learning rate randomly
    new_lr = float(min(1., max(0., model.learning_rate+(model.learning_rate*uniform(-.1, .1)))))
    #new_n_steps = int(max(0., model.n_steps+(model.n_steps*uniform(-.1, .1))))
    new_batch_size = int(max(0., model.batch_size+(model.batch_size*uniform(-.1, .1))))
    new_gamma = float(min(max(0.,model.gamma+(model.gamma*uniform(-.001, .001))), .99999999))
    model = PPO.load(src_model_file_path, 
                    learning_rate=new_lr, 
                    n_steps=new_batch_size,
                    batch_size=new_batch_size,
                    gamma=new_gamma)
    # End Exploration
    new_model_file_path = args.model_dir + name + "/" + name + "_0"
    model.save(new_model_file_path)
    new_model_stats = init_stats()
    new_model_stats["parent"] = remove_steps(source_agent[ID])
    new_model_stats["elo"]["value"][-1] = source_agent[ELO]
    with open(args.model_dir + name + "/stats.json", 'w') as new_model_stats_file:
        json.dump(new_model_stats, new_model_stats_file, indent=4)
# Child agent nemesis
    if gen_nemesis:
        src_model_nemesis_file_path = args.model_dir + remove_steps(source_agent[ID]) + "-nemesis/" + remove_steps(source_agent[ID]) + "-nemesis_" + source_agent[ID].split('_')[-1]
        nemesis = PPO.load(src_model_nemesis_file_path,
                            learning_rate=new_lr,
                            n_steps=new_batch_size,
                            batch_size=new_batch_size,
                            gamma=new_gamma)
        new_nemesis_file_path = args.model_dir + nemesis_name + "/" + nemesis_name + "_0"
        nemesis.save(new_nemesis_file_path)
        new_nemesis_stats = init_stats()
        new_nemesis_stats["matching_agent"] = name
        new_nemesis_stats["nemesis"] = True
        new_nemesis_stats["elo"]["value"][-1] = source_agent[ELO]
        with open(args.model_dir + nemesis_name + "/stats.json", 'w') as new_nemesis_stats_file:
            json.dump(new_nemesis_stats, new_nemesis_stats_file, indent=4)
    if gen_survivor:
        src_model_survivor_file_path = args.model_dir + remove_steps(source_agent[ID]) + "-survivor/" + remove_steps(source_agent[ID]) + "-survivor_" + source_agent[ID].split('_')[-1]
        survivor = PPO.load(src_model_survivor_file_path,
                            learning_rate=new_lr,
                            n_steps=new_batch_size,
                            batch_size=new_batch_size,
                            gamma=new_gamma)
        new_survivor_file_path = args.model_dir + survivor_name + "/" + survivor_name + "_0"
        nemesis.save(new_survivor_file_path)
        new_survivor_stats = init_stats()
        new_survivor_stats["matching_agent"] = name
        new_survivor_stats["survivor"] = True
        new_survivor_stats["elo"]["value"][-1] = source_agent[ELO]
        with open(args.model_dir + survivor_name + "/stats.json", 'w') as new_survivor_stats_file:
            json.dump(new_survivor_stats, new_survivor_stats_file, indent=4)
        
    return (name, nemesis_name, survivor_name)
    
    
if not (args.model_dir[-1] == '/' or args.model_dir[-1] == '\\'):
    args.model_dir = args.model_dir + "/"
    
if not os.path.exists(args.game_path):
    raise FileNotFoundError("Inputted game path does not lead to an existing file")
    
if not os.path.exists(args.game_config_file_path):
    raise FileNotFoundError("Game config file not found")
    
if not os.path.exists(args.eval_script):
    raise FileNotFoundError("Python Evaluation script not found")
    
if not os.path.isdir(args.model_dir):
    raise FileNotFoundError("Base directory for agent models is not a folder")
    
if not os.path.exists(args.pop_file_path):
    raise FileNotFoundError("Inputted path does not lead to population file")
    
if not os.path.exists(args.noun_file_path):
    raise FileNotFoundError("Inputted path does not lead to noun file")
    
if not os.path.exists(args.adj_file_path):
    raise FileNotFoundError("Inputted path does not lead to adjective file")
    
population = []
HAS_NEM = False
HAS_SURV = False
with open(args.pop_file_path, 'r') as pop_file:
    for line in pop_file.readlines():
        if "-nemesis" in line:
            HAS_NEM = True
        elif "-survivor" in line:
            HAS_SURV = True
        population.append(line.strip('\n'))
        
pop_elos = []
for i,p in enumerate(population):
    if not os.path.isdir(args.model_dir + p):
        raise FileNotFoundError("Agent ID {" + p + "} does not lead to a valid model directory")
    p_stats_path = args.model_dir + p + "/stats.json"
    if not os.path.exists(p_stats_path):
        raise FileNotFoundError("Agent ID (" + p + ") does not have a stats file")
    with open(p_stats_path, 'r') as p_stats_file:
        p_stats = json.load(p_stats_file)
    p_step = p_stats["num_steps"]
    p_elo = p_stats["elo"]["value"][-1]
    if not os.path.exists(args.model_dir + p + "/" + p + "_" + str(p_step) + ".zip"):
        raise FileNotFoundError("Agent ID (" + p + ") does not have a correct saved policy file")
    population[i] += "_" + str(p_step)
    pop_elos.append(p_elo)
      
pop_with_elos = [x for x in zip(population, pop_elos)]
print("Current population:", pop_with_elos)
pairs = [(a,b) for idx,a in enumerate(pop_with_elos) for b in pop_with_elos[idx+1:]]
shuffle(pairs)
print("Order of evaluation:")
for (agent_a, agent_b) in pairs:
    if "-nemesis" in agent_a[ID] or "-nemesis" in agent_b[ID]: 
        continue
    if "-survivor" in agent_a[ID] or "-survivor" in agent_b[ID]: 
        continue
    print('\t' + str((agent_a, agent_b)))
    
replaced = []
for (agent_a, agent_b) in pairs:
    if "-nemesis" in agent_a[ID] or "-nemesis" in agent_b[ID]: 
        continue
    if "-survivor" in agent_a[ID] or "-survivor" in agent_b[ID]: 
        continue
    print(agent_a, "compared to", agent_b)
    # If one agent has already been replaced, skip
    if agent_a[ID] in replaced or agent_b[ID] in replaced:
        print(agent_a[ID] if agent_a[ID] in replaced else agent_b, "has already been replaced")
        continue
    # If one agent has not met the minimum step requirement, skip
    if int(agent_a[ID].split('_')[-1]) < args.min_step or int(agent_b[ID].split('_')[-1]) < args.min_step:
        print(agent_a[ID] if int(agent_a[ID].split('_')[-1]) < args.min_step else agent_b[ID], "does not have the minimum number of steps")
        continue
    # Calculate win probability of agent_a vs agent_b
    win_prob = elo_prob(agent_a[ELO], agent_b[ELO])
    # Agent_a should be the agent that should win, should either agent have a chance of replacing the other
    if win_prob < .5:
        tmp = agent_a
        agent_a = agent_b
        agent_b = tmp
        win_prob = 1 - win_prob
    # Evaluate agent_a vs agent_b if agent_a is expected to win over a threshold probability
    if win_prob > args.win_thresh:
        print("Evaluating", agent_a[ID], "vs", agent_b[ID], "for replacement")
        agent_a_win_rate = evaluate(agent_a, agent_b)#(evaluate(agent_a, agent_b) + (1-evaluate(agent_b, agent_a)))/2.
        if agent_a_win_rate > args.win_thresh:
            print("Replacing", agent_b[ID], "with", agent_a[ID], "due to win rate", agent_a_win_rate)
            (name, nemesis_name, survivor_name) = child_agent(agent_a, gen_nemesis=HAS_NEM, gen_survivor=HAS_SURV)
            print("\t--New name:", name)
            replaced.append(agent_b[ID])
            population.remove(agent_b[ID])
            population.append(name + "_0")
            if HAS_NEM:
                population.remove(remove_steps(agent_b[ID]) + "-nemesis_" + agent_b[ID].split('_')[-1])
                population.append(nemesis_name + "_0")
            if HAS_SURV:
                population.remove(remove_steps(agent_b[ID]) + "-survivor_" + agent_b[ID].split('_')[-1])
                population.append(survivor_name + "_0")
        else:
            print(agent_a[ID], "was not able to beat", agent_b[ID], "with a win rate above threshold", args.win_thresh)
            print("\t--Evaluated win rate:", agent_a_win_rate)
    else:
        print("Win prob of", win_prob, "is not enough to evaluate", agent_a[ID], "against", agent_b[ID])
if len(replaced):
    print("New Population:", population)
else:
    print("No replacements in population")
with open(args.pop_file_path, 'w') as pop_file:
    for p in population:
        pop_file.write(remove_steps(p) + '\n')